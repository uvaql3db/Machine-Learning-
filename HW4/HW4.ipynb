{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GridWorld import GridWorld\n",
    "from ValueIteration import ValueIteration\n",
    "from PolicyIteration import PolicyIteration\n",
    "import numpy as np\n",
    "\n",
    "problem = GridWorld('world00.csv',reward={0: -0.04, 1: 1.0, 2: -1.0, 3: np.NaN}, random_rate=0.2)\n",
    "\n",
    "solver = ValueIteration(problem.reward_function, problem.transition_model, gamma=0.9)\n",
    "solver.train()\n",
    "\n",
    "problem.visualize_value_policy(policy=solver.policy, values=solver.values)\n",
    "# problem.random_start_policy(policy=solver.policy, start_pos=[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = GridWorld('world00.csv',reward={0: -0.04, 1: 1.0, 2: -1.0, 3: np.NaN}, random_rate=0.2)\n",
    "policy = [1, 1, 3, 1, 0, 0, 2, 0, 1, 2, 1, 0]\n",
    "\n",
    "solver = PolicyIteration(problem.reward_function, problem.transition_model, gamma=0.9, init_policy=policy)\n",
    "solver.train()\n",
    "\n",
    "problem.visualize_value_policy(policy=solver.policy, values=solver.values)\n",
    "# problem.random_start_policy(policy=solver.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import sys\n",
    "from mdp import MDP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(' .')\n",
    "gym.envs.register('Gambler-v1', entry_point='gambler:GamblerEnv', max_episode_steps=1000)\n",
    "\n",
    "gambler = MDP(environment='Gambler-v1', convergence_threshold=0.00001, grid=False)\n",
    "print('Gambler Problem:')\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambler.value_iteration(iterations_to_save=[8], visualize=False)\n",
    "optimal_action_vi = gambler.optimal_policy_actions.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(6,4), dpi=300)\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Amount of bet')\n",
    "ax.plot(optimal_action_vi)\n",
    "ax.set_title('Gambler - Best action in each state')\n",
    "plt.show()\n",
    "\n",
    "# plt.plot(optimal_action_vi)\n",
    "# plt.show()\n",
    "print(optimal_action_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambler.policy_iteration(iterations_to_save=[1, 8, 16], visualize=False)\n",
    "optimal_action_pi = gambler.optimal_policy_actions.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,6), dpi=300)\n",
    "ax.set_xlabel('State')\n",
    "ax.set_ylabel('Amount of bet')\n",
    "ax.plot(optimal_action_pi)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Q-learning\n",
    "#import libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "#define the shape of the environment (i.e., its states)\n",
    "environment_rows = 3\n",
    "environment_columns = 4\n",
    "#Create a 3D numpy array to hold the current Q-values for each state and action pair: Q(s, a) \n",
    "#The array contains 3 rows and 4 columns (to match the shape of the environment), as well as a third \"action\" dimension.\n",
    "#The \"action\" dimension consists of 4 layers that will allow us to keep track of the Q-values for each possible action in\n",
    "#each state (see next cell for a description of possible actions). \n",
    "#The value of each (state, action) pair is initialized to 0.\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "\n",
    "#define actions\n",
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "\n",
    "#Create a 2D numpy array to hold the rewards for each state. \n",
    "#The array contains 11 rows and 11 columns (to match the shape of the environment), and each value is initialized to -100.\n",
    "rewards = np.full((environment_rows, environment_columns), -0.04)\n",
    "rewards[0,3] = 1\n",
    "rewards[1,3] = -1\n",
    "rewards[1,1] = -100\n",
    "\n",
    "for row in rewards:\n",
    "    print(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is -0.04, then it is not a terminal state (i.e., it is a 'white square')\n",
    "    if rewards[current_row_index, current_column_index] == -0.04:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "    \n",
    "#define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "    return 2,0\n",
    "    #get a random row and column index\n",
    "#     current_row_index = np.random.randint(environment_rows)\n",
    "#     current_column_index = np.random.randint(environment_columns)\n",
    "#     #continue choosing random row and column indexes until a non-terminal state is identified\n",
    "#     #(i.e., until the chosen state is a 'white square').\n",
    "#     while is_terminal_state(current_row_index, current_column_index):\n",
    "#         current_row_index = np.random.randint(environment_rows)\n",
    "#         current_column_index = np.random.randint(environment_columns)\n",
    "#     return current_row_index, current_column_index\n",
    "\n",
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "    #if a randomly chosen value between 0 and 1 is less than epsilon, \n",
    "    #then choose the most promising value from the Q-table for this state.\n",
    "    if np.random.random() < epsilon:\n",
    "        return np.argmax(q_values[current_row_index, current_column_index])\n",
    "    else: #choose a random action\n",
    "        return np.random.randint(4)\n",
    "\n",
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "    new_row_index = current_row_index\n",
    "    new_column_index = current_column_index\n",
    "    if actions[action_index] == 'up' and current_row_index > 0:\n",
    "        new_row_index -= 1\n",
    "    elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "        new_column_index += 1\n",
    "    elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "        new_row_index += 1\n",
    "    elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "        new_column_index -= 1\n",
    "    if (new_row_index ==2 and new_column_index == 0):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "    return new_row_index, new_column_index\n",
    "\n",
    "#Define a function that will get the shortest path between any location within the warehouse that \n",
    "#the robot is allowed to travel and the item packaging location.\n",
    "def get_shortest_path(start_row_index, start_column_index):\n",
    "  #return immediately if this is an invalid starting location\n",
    "    if is_terminal_state(start_row_index, start_column_index):\n",
    "        return []\n",
    "    else: #if this is a 'legal' starting location\n",
    "        current_row_index, current_column_index = start_row_index, start_column_index\n",
    "        shortest_path = []\n",
    "        shortest_path.append([current_row_index, current_column_index])\n",
    "        #continue moving along the path until we reach the goal (i.e., the item packaging location)\n",
    "        while not is_terminal_state(current_row_index, current_column_index):\n",
    "            #get the best action to take\n",
    "            action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "            #move to the next location on the path, and add the new location to the list\n",
    "            current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "        return shortest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define training parameters\n",
    "\n",
    "start = time.time()\n",
    "epsilon = 0.01 #the percentage of time when we should take the best action (instead of a random action)\n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the agent should learn\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "    #get the starting location for this episode\n",
    "    row_index, column_index = get_starting_location()\n",
    "    #continue taking actions (i.e., moving) until we reach a terminal state\n",
    "    #(i.e., until we reach the item packaging area or crash into an item storage location)\n",
    "    while not is_terminal_state(row_index, column_index):\n",
    "#         print(row_index,column_index)\n",
    "        #choose which action to take (i.e., where to move next)\n",
    "        action_index = get_next_action(row_index, column_index, epsilon)\n",
    "        #perform the chosen action, and transition to the next state (i.e., move to the next location)\n",
    "        old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "        row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "        #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "        reward = rewards[row_index, column_index]\n",
    "        old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "        temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "        #update the Q-value for the previous state and action pair\n",
    "        new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "        q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "\n",
    "print('Training complete!',total_time)\n",
    "print(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_value_policy(q_values,fig_size=(8, 6)):\n",
    "    num_rows = 3\n",
    "    num_cols = 4\n",
    "    unit = min(fig_size[1] // num_rows, fig_size[0] // num_cols)\n",
    "    unit = max(1, unit)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=fig_size)\n",
    "    ax.axis('off')\n",
    "\n",
    "    for i in range(num_cols + 1):\n",
    "        if i == 0 or i == num_cols:\n",
    "            ax.plot([i * unit, i * unit], [0, num_rows * unit],\n",
    "                    color='black')\n",
    "        else:\n",
    "            ax.plot([i * unit, i * unit], [0, num_rows * unit],\n",
    "                    alpha=0.7, color='grey', linestyle='dashed')\n",
    "    for i in range(num_rows + 1):\n",
    "        if i == 0 or i == num_rows:\n",
    "            ax.plot([0, num_cols * unit], [i * unit, i * unit],\n",
    "                    color='black')\n",
    "        else:\n",
    "            ax.plot([0, num_cols * unit], [i * unit, i * unit],\n",
    "                    alpha=0.7, color='grey', linestyle='dashed')\n",
    "\n",
    "        for i in range(num_rows):\n",
    "            for j in range(num_cols):\n",
    "                y = (num_rows - 1 - i) * unit\n",
    "                x = j * unit\n",
    "#                 s = q_values((i, j))\n",
    "                if i == 1 and j == 1:\n",
    "                    rect = patches.Rectangle((x, y), unit, unit, edgecolor='none', facecolor='black',\n",
    "                                             alpha=0.6)\n",
    "                    ax.add_patch(rect)\n",
    "                elif i ==1 and j ==3:\n",
    "                    rect = patches.Rectangle((x, y), unit, unit, edgecolor='none', facecolor='red',\n",
    "                                             alpha=0.6)\n",
    "                    ax.add_patch(rect)\n",
    "                elif i == 0 and j == 3:\n",
    "                    rect = patches.Rectangle((x, y), unit, unit, edgecolor='none', facecolor='green',\n",
    "                                             alpha=0.6)\n",
    "                    ax.add_patch(rect)\n",
    "                else:\n",
    "                    ax.text(x + 0.5 * unit, y + 0.5 * unit, f'{max(q_values[i][j]):.4f}',\n",
    "                            horizontalalignment='center', verticalalignment='center',\n",
    "                            fontsize=max(fig_size)*unit*0.6)\n",
    "                    symbol = ['^', '>', 'v', '<']\n",
    "                    a = np.argmax(q_values[i][j])\n",
    "                    ax.plot([x + 0.5 * unit], [y + 0.5 * unit], marker=symbol[a], alpha=0.4,\n",
    "                                linestyle='none', markersize=max(fig_size)*unit, color='#1f77b4')\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "visualize_value_policy(q_values=q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gambler.Q_learning(num_episodes=10000, learning_rate_decay=0.995, epsilon_decay=0.995, visualize=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
